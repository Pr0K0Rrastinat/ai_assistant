import requests
import time
import re
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from scripts.extrect_object_category import group_norms_by_category

def dedup_text_norms(norms):
    seen = set()
    deduped = []
    for norm in norms:
        text = norm.get("text", "")
        full_id = norm.get("full_id", "")
        key = f"{text}:{full_id}"
        if text and key not in seen:
            deduped.append(norm)
            seen.add(key)
    return deduped


def dedup_table_norms(norms):
    seen = set()
    deduped = []
    for norm in norms:
        indicator = norm.get("indicator", "")
        values_str = json.dumps(norm.get("values", {}), ensure_ascii=False)
        full_id = norm.get("full_id", "")
        key = f"{indicator}:{values_str}:{full_id}"
        if key not in seen:
            deduped.append(norm)
            seen.add(key)
    return deduped


def split_into_batches(all_norms, batch_size):
    for i in range(0, len(all_norms), batch_size):
        yield all_norms[i:i + batch_size]

def format_table_norm(norm):
    indicator = norm.get("indicator", "‚Äî")
    values = norm.get("values", {})
    result = f"‚óæ {indicator}:\n"
    for cls, val in values.items():
        result += f"   - {cls}: {val}\n"
    return result.strip()

def format_text_norm(norm):
    text = norm.get("text", "").strip()
    norm = norm.get("full_id","").strip()
    return f"{norm}- {text}"

def clean_llm_output(text):
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()

def generate_combined_prompt(text_norms, table_norms, user_question):
    parts = []

    if table_norms:
        table_part = "\n\n".join([format_table_norm(n) for n in table_norms])
        parts.append(f"üìä –¢–∞–±–ª–∏—á–Ω—ã–µ –Ω–æ—Ä–º—ã:\n{table_part}")

    if text_norms:
        text_part = "\n".join([format_text_norm(n) for n in text_norms])
        parts.append(f"üìú –¢–µ–∫—Å—Ç–æ–≤—ã–µ –Ω–æ—Ä–º—ã:\n{text_part}")

    prompt = (
        f"–í–æ–ø—Ä–æ—Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞:\n{user_question.strip()}\n\n" +
        "–ù–æ—Ä–º—ã –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–≤–µ—Ä–∏—Ç—å"+
        "\n\n".join(parts) +
        "\n\n–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤,–ø–æ–ª–µ–∑–Ω—ã–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.–ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–ù–û  –ò–°–ü–û–õ–¨–ó–£–ô –¢–û–õ–¨–ö–û –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–æ—Ä–º—ã –Ω–µ –±–µ—Ä–∏ –¥–∞–Ω–Ω—ã–µ —Å–æ —Å–≤–æ–µ–π –±–∞–∑–∑—ã "
    )

    return prompt

def generate_categorized_prompt(text_norms, table_norms, user_question,sourse):
    prompt_parts = [f"–í–æ–ø—Ä–æ—Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞:\n{user_question.strip()}"]

    if text_norms:
        grouped_text = group_norms_by_category(text_norms)
        for category, norms in grouped_text.items():
            formatted = "\n".join([format_text_norm(n) for n in norms])
            prompt_parts.append(f"üìú {category} (–¢–µ–∫—Å—Ç–æ–≤—ã–µ –Ω–æ—Ä–º—ã):\n{formatted}")

    if table_norms:
        grouped_table = group_norms_by_category(table_norms)
        for category, norms in grouped_table.items():
            formatted = "\n\n".join([format_table_norm(n) for n in norms])
            prompt_parts.append(f"üìä {category} (–¢–∞–±–ª–∏—á–Ω—ã–µ –Ω–æ—Ä–º—ã):\n{formatted}")

    prompt_parts.append(f"\n–ù–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–ª—å–∫–æ —ç—Ç–∏—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤, –¥–∞–π –∫—Ä–∞—Ç–∫–∏–π, –ø–æ–ª–µ–∑–Ω—ã–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.–î–æ–∫—É–º–µ–Ω—Ç –∫–æ—Ç–æ—Ä—ã–π —Ç—ã —á–∏—Ç–∞–µ—à—å {sourse}.–ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–ù–û –ò–°–ü–û–õ–¨–ó–£–ô –¢–û–õ–¨–ö–û –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–æ—Ä–º—ã –Ω–µ –±–µ—Ä–∏ –¥–∞–Ω–Ω—ã–µ —Å–æ —Å–≤–æ–µ–π –±–∞–∑—ã. –¢–∞–º —É–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ–ª–Ω—ã–π –ø—É–Ω–∫—Ç –∫ –Ω–æ—Ä–º–∞–º –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö –∫–æ–≥–¥–∞ –±—É–¥–µ—à—å –ø–∏—Å–∞—Ç—å –æ—Ç–≤–µ—Ç")
    return "\n\n".join(prompt_parts)



"""def summarize_llm_batches(responses, question, model_name="gemma"):
    if not responses:
        return "‚ùó –ù–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è."

    combined = "\n\n".join([f"–û—Ç–≤–µ—Ç {i+1}:\n{resp}" for i, resp in enumerate(responses)])
    prompt = (
    f"–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞. –ï—Å–ª–∏ —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∏ –∫–ª–∞—Å—Å —Å–∏–Ω–æ–Ω–∏–º—ã. –ï—Å–ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ —Å–ø—Ä—à–∏–≤–∞—é—Ç –Ω–∞—Å—á–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –∫–ª–∞—Å—Å —Ç–æ —Å—á–∏—Ç–∞–π —á—Ç–æ —ç—Ç–æ –æ–¥–Ω–æ –∏ —Ç–æ–∂–µ –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —Å—Ç—Ä–æ–≥–æ –ø–æ –Ω–æ—Ä–º–∞–º.\n\n"
    f"–í–æ–ø—Ä–æ—Å:\n{question.strip()}\n\n"
    f"{combined}\n\n"
    "**üìå –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç:**"
)



    try:
        response = requests.post("http://localhost:11434/api/generate", json={
            "model": model_name,
            "prompt": prompt,
            "stream": False
        })
        response.raise_for_status()
        return response.json().get("response", "‚ùå –ù–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.")
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}"
"""

def summarize_llm_batches(responses, question, model_name="qwen3"):
    if not responses:
        return "‚ùó –ù–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è."

    combined = "\n\n".join([f"–û—Ç–≤–µ—Ç {i+1}:\n{resp}" for i, resp in enumerate(responses)])
    prompt = (
        "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞.–ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–ù–û —á—Ç–æ–±—ã –æ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –Ω–æ—Ä–º—ã —Ç–æ–ª—å–∫–æ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞.–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –°–Ω–ø—ã –∏–ª–∏ –†–æ—Å—Å–∏—Å–∫–∏–µ –Ω–æ—Ä–º—ã.\n\n"
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞—Ç—å –∫—Ä–∞—Ç–∫–∏–π, —á—ë—Ç–∫–∏–π –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–π –≤—ã–≤–æ–¥ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.–¢–∞–º —É–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –ø—É–Ω–∫—Ç –∫ –Ω–æ—Ä–º–∞–º –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö –∫–æ–≥–¥–∞ –±—É–¥–µ—à—å –ø–∏—Å–∞—Ç—å –æ—Ç–≤–µ—Ç\n"
        f"–í–æ–ø—Ä–æ—Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞:\n{question.strip()}\n\n"
        f"{combined}\n\n"
        "**üìå –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç, —á—ë—Ç–∫–æ —É–∫–∞–∑–∞–≤ –Ω–æ—Ä–º—ã –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–∏–ø—É –æ–±—ä–µ–∫—Ç–∞.**"
    )

    return call_llm(prompt, model_name=model_name)

def call_llm(prompt, model_name="mistral"):
    try:
        response = requests.post("http://localhost:11434/api/generate", json={
            "model": model_name,
            "prompt": prompt,
            "stream": False
        })
        response.raise_for_status()
        return clean_llm_output(response.json().get("response", "‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏"))
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –º–æ–¥–µ–ª–∏: {e}"

def check_multi_norms_mistral_nemo_parallel4(
    text_norms: list,
    table_norms: list,
    fact_text: str,
    sourse: str,
    progress_bar=None,
    progress_label=None
):
    model_name = "gpt-oss:20b"
    print(fact_text)
    def update_progress(step, label):
        if progress_bar:
            progress_bar.progress(step)
        if progress_label:
            progress_label.text(f"{label} ‚Äî {int(step * 100)}%")

    text_norms = dedup_text_norms(text_norms)
    table_norms = dedup_table_norms(table_norms)

    # === –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ 2 —á–∞—Å—Ç–∏ ===
    text_batches = list(split_into_batches(text_norms, max(1, len(text_norms)//2)))
    table_batches = list(split_into_batches(table_norms, max(1, len(table_norms)//2)))

    all_prompts = []
    for batch in text_batches:
        all_prompts.append(generate_categorized_prompt(batch, [], fact_text,sourse))
    for batch in table_batches:
        all_prompts.append(generate_categorized_prompt([], batch, fact_text,sourse))

    all_responses = []
    update_progress(0.0, "–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(call_llm, prompt, model_name) for prompt in all_prompts]
        for i, future in enumerate(as_completed(futures)):
            try:
                result = future.result()
                all_responses.append(result)
                update_progress((i+1)/5, f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i+1} –∏–∑ 4 –±–ª–æ–∫–æ–≤")
            except Exception as e:
                all_responses.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –º–æ–¥–µ–ª–∏: {e}")

    final_answer = summarize_llm_batches(all_responses, fact_text, model_name)
    update_progress(1.0, "–§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –≥–æ—Ç–æ–≤")
    return final_answer
