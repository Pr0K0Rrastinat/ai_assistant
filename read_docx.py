import requests
import time
import re
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
def dedup_text_norms(norms):
    seen = set()
    deduped = []
    for norm in norms:
        text = norm.get("text", "")
        if text and text not in seen:
            deduped.append(norm)
            seen.add(text)
    return deduped

def dedup_table_norms(norms):
    seen = set()
    deduped = []
    for norm in norms:
        indicator = norm.get("indicator", "")
        values_str = json.dumps(norm.get("values", {}), ensure_ascii=False)
        key = f"{indicator}:{values_str}"
        if key not in seen:
            deduped.append(norm)
            seen.add(key)
    return deduped

def split_into_batches(all_norms, batch_size):
    for i in range(0, len(all_norms), batch_size):
        yield all_norms[i:i + batch_size]

def format_table_norm(norm):
    indicator = norm.get("indicator", "‚Äî")
    values = norm.get("values", {})
    result = f"‚óæ {indicator}:\n"
    for cls, val in values.items():
        result += f"   - {cls}: {val}\n"
    return result.strip()

def format_text_norm(norm):
    text = norm.get("text", "").strip()
    return f"- {text}"

def clean_llm_output(text):
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()

def generate_combined_prompt(text_norms, table_norms, user_question):
    parts = []

    if table_norms:
        table_part = "\n\n".join([format_table_norm(n) for n in table_norms])
        parts.append(f"üìä –¢–∞–±–ª–∏—á–Ω—ã–µ –Ω–æ—Ä–º—ã:\n{table_part}")

    if text_norms:
        text_part = "\n".join([format_text_norm(n) for n in text_norms])
        parts.append(f"üìú –¢–µ–∫—Å—Ç–æ–≤—ã–µ –Ω–æ—Ä–º—ã:\n{text_part}")

    prompt = (
        f"–í–æ–ø—Ä–æ—Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞:\n{user_question.strip()}\n\n" +
        "\n\n".join(parts) +
        "\n\n–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤, –¥–∞–π –∫—Ä–∞—Ç–∫–∏–π, –ø–æ–ª–µ–∑–Ω—ã–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
    )

    return prompt


"""def summarize_llm_batches(responses, question, model_name="gemma"):
    if not responses:
        return "‚ùó –ù–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è."

    combined = "\n\n".join([f"–û—Ç–≤–µ—Ç {i+1}:\n{resp}" for i, resp in enumerate(responses)])
    prompt = (
    f"–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞. –ï—Å–ª–∏ —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∏ –∫–ª–∞—Å—Å —Å–∏–Ω–æ–Ω–∏–º—ã. –ï—Å–ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ —Å–ø—Ä—à–∏–≤–∞—é—Ç –Ω–∞—Å—á–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –∫–ª–∞—Å—Å —Ç–æ —Å—á–∏—Ç–∞–π —á—Ç–æ —ç—Ç–æ –æ–¥–Ω–æ –∏ —Ç–æ–∂–µ –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —Å—Ç—Ä–æ–≥–æ –ø–æ –Ω–æ—Ä–º–∞–º.\n\n"
    f"–í–æ–ø—Ä–æ—Å:\n{question.strip()}\n\n"
    f"{combined}\n\n"
    "**üìå –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç:**"
)



    try:
        response = requests.post("http://localhost:11434/api/generate", json={
            "model": model_name,
            "prompt": prompt,
            "stream": False
        })
        response.raise_for_status()
        return response.json().get("response", "‚ùå –ù–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.")
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}"
"""

def summarize_llm_batches(responses, question, model_name="qwen3"):
    if not responses:
        return "‚ùó –ù–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è."

    combined = "\n\n".join([f"–û—Ç–≤–µ—Ç {i+1}:\n{resp}" for i, resp in enumerate(responses)])
    prompt = (
        f"–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —Å—Ç—Ä–æ–≥–æ –ø–æ –Ω–æ—Ä–º–∞–º.\n\n"
        f"–í–æ–ø—Ä–æ—Å:\n{question.strip()}\n\n"
        f"{combined}\n\n"
        "**üìå –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç:**"
    )

    return call_llm(prompt, model_name=model_name)

def call_llm(prompt, model_name="mistral"):
    try:
        response = requests.post("http://localhost:11434/api/generate", json={
            "model": model_name,
            "prompt": prompt,
            "stream": False
        })
        response.raise_for_status()
        return clean_llm_output(response.json().get("response", "‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏"))
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –º–æ–¥–µ–ª–∏: {e}"

def check_multi_norms_combined_with_llama_parallel(
    text_norms: list,
    table_norms: list,
    fact_text: str,
    model_name="mistral",
    max_workers=4,
    progress_bar=None,
    progress_label=None
):
    if not text_norms and not table_norms:
        return "‚ùó –ù–æ—Ä–º–∞—Ç–∏–≤—ã –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã, –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞."

    # === –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    text_norms = dedup_text_norms(text_norms)
    table_norms = dedup_table_norms(table_norms)

    all_responses = []
    start_time = time.time()

    # === –¢–ï–ö–°–¢–û–í–´–ï –ù–û–†–ú–´ ===
    def process_text_batch(batch):
        prompt = generate_combined_prompt(batch, [], fact_text)
        return call_llm(prompt, model_name=model_name)

    text_batches = list(split_into_batches(text_norms, 8))
    print(f"üìù –¢–µ–∫—Å—Ç–æ–≤—ã—Ö –Ω–æ—Ä–º: {len(text_norms)}, –±–∞—Ç—á–µ–π: {len(text_batches)}")
    table_batches = list(split_into_batches(table_norms, 8))
    print(f"üìä –¢–∞–±–ª–∏—á–Ω—ã—Ö –Ω–æ—Ä–º: {len(table_norms)}, –±–∞—Ç—á–µ–π: {len(table_batches)}")
    total_batches = len(text_batches) + len(table_batches)
    progress_count = 0
    def update_progress(label=None):
        nonlocal progress_count
        progress_count += 1
        fraction = progress_count / total_batches
        if progress_bar:
            progress_bar.progress(fraction)
        if progress_label and label:
            percent = int(fraction * 100)
            progress_label.text(f"{label} ‚Äî {percent}%")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_text_batch, batch): i for i, batch in enumerate(text_batches, 1)}
        for future in as_completed(futures):
            idx = futures[future]
            try:
                result = future.result()
                print("====")
                print(result)
                all_responses.append(result)
                update_progress()
                print(f"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π –±–∞—Ç—á {idx} –∑–∞–≤–µ—Ä—à—ë–Ω.")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –±–∞—Ç—á–µ {idx}: {e}")
                all_responses.append(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –±–∞—Ç—á–µ {idx}: {e}")


    # === –¢–ê–ë–õ–ò–ß–ù–´–ï –ù–û–†–ú–´ ===
    def process_table_batch(batch):
        prompt = generate_combined_prompt([], batch, fact_text)
        return call_llm(prompt, model_name=model_name)


    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_table_batch, batch): i for i, batch in enumerate(table_batches, 1)}
        for future in as_completed(futures):
            idx = futures[future]
            try:
                result = future.result()
                print("====")
                print(result)
                all_responses.append(result)
                print(f"‚úÖ –¢–∞–±–ª–∏—á–Ω—ã–π –±–∞—Ç—á {idx} –∑–∞–≤–µ—Ä—à—ë–Ω.")
                update_progress()
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–∞–±–ª–∏—á–Ω–æ–º –±–∞—Ç—á–µ {idx}: {e}")
                all_responses.append(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–∞–±–ª–∏—á–Ω–æ–º –±–∞—Ç—á–µ {idx}: {e}")

    # === –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
    elapsed = time.time() - start_time
    print(f"\nüèÅ –í—Å–µ –±–∞—Ç—á–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∑–∞ {round(elapsed, 2)} —Å–µ–∫.")
    print(f"–í–æ–ø—Ä–æ—Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞: {fact_text}")

    if all_responses:
        return clean_llm_output(summarize_llm_batches(all_responses, fact_text, model_name=model_name))
    else:
        return "‚ùó –ù–µ—Ç –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."