import requests
import time
import re
import json

def dedup_text_norms(norms):
    seen = set()
    deduped = []
    for norm in norms:
        text = norm.get("text", "")
        if text and text not in seen:
            deduped.append(norm)
            seen.add(text)
    return deduped

def dedup_table_norms(norms):
    seen = set()
    deduped = []
    for norm in norms:
        indicator = norm.get("indicator", "")
        values_str = json.dumps(norm.get("values", {}), ensure_ascii=False)
        key = f"{indicator}:{values_str}"
        if key not in seen:
            deduped.append(norm)
            seen.add(key)
    return deduped

def split_into_batches(all_norms, batch_size):
    for i in range(0, len(all_norms), batch_size):
        yield all_norms[i:i + batch_size]

def format_table_norm(norm):
    indicator = norm.get("indicator", "‚Äî")
    values = norm.get("values", {})
    result = f"‚óæ {indicator}:\n"
    for cls, val in values.items():
        result += f"   - {cls}: {val}\n"
    return result.strip()

def format_text_norm(norm):
    text = norm.get("text", "").strip()
    return f"- {text}"

def generate_combined_prompt(text_norms, table_norms, user_question):
    parts = []

    if table_norms:
        table_part = "\n\n".join([format_table_norm(n) for n in table_norms])
        parts.append(f"üìä –¢–∞–±–ª–∏—á–Ω—ã–µ –Ω–æ—Ä–º—ã:\n{table_part}")

    if text_norms:
        text_part = "\n".join([format_text_norm(n) for n in text_norms])
        parts.append(f"üìú –¢–µ–∫—Å—Ç–æ–≤—ã–µ –Ω–æ—Ä–º—ã:\n{text_part}")

    prompt = (
        f"–í–æ–ø—Ä–æ—Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞:\n{user_question.strip()}\n\n" +
        "\n\n".join(parts) +
        "\n\n–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤, –¥–∞–π –∫—Ä–∞—Ç–∫–∏–π, –ø–æ–ª–µ–∑–Ω—ã–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
    )

    return prompt


def summarize_llm_batches(responses, question, model_name="qwen3"):
    if not responses:
        return "‚ùó –ù–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è."

    combined = "\n\n".join([f"–û—Ç–≤–µ—Ç {i+1}:\n{resp}" for i, resp in enumerate(responses)])
    prompt = (
    f"–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞. –ï—Å–ª–∏ —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∏ –∫–ª–∞—Å—Å —Å–∏–Ω–æ–Ω–∏–º—ã. –ï—Å–ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ —Å–ø—Ä—à–∏–≤–∞—é—Ç –Ω–∞—Å—á–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –∫–ª–∞—Å—Å —Ç–æ —Å—á–∏—Ç–∞–π —á—Ç–æ —ç—Ç–æ –æ–¥–Ω–æ –∏ —Ç–æ–∂–µ –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —Å—Ç—Ä–æ–≥–æ –ø–æ –Ω–æ—Ä–º–∞–º.\n\n"
    f"–í–æ–ø—Ä–æ—Å:\n{question.strip()}\n\n"
    f"{combined}\n\n"
    "**üìå –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç:**"
)



    try:
        response = requests.post("http://localhost:11434/api/generate", json={
            "model": model_name,
            "prompt": prompt,
            "stream": False
        })
        response.raise_for_status()
        return response.json().get("response", "‚ùå –ù–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.")
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}"


def clean_llm_output(text):
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()

def check_multi_norms_combined_with_llama(
    text_norms: list,
    table_norms: list,
    fact_text: str,
    model_name="qwen3",
    progress_bar=None,
    progress_label=None
):
    if not text_norms and not table_norms:
        return "‚ùó –ù–æ—Ä–º–∞—Ç–∏–≤—ã –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã, –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞."

    text_norms = dedup_text_norms(text_norms)
    table_norms = dedup_table_norms(table_norms)

    # –°–ª–∏–≤–∞–µ–º –≤ –æ–¥–∏–Ω —Å–ø–∏—Å–æ–∫, –±–∞—Ç—á–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ –¥–µ–ª–∏–º
    all_pairs = list(zip(text_norms, [None]*len(text_norms))) + list(zip([None]*len(table_norms), table_norms))
    batches = list(split_into_batches(all_pairs, 8))
    total = len(batches)

    print(f"üì¶ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(all_pairs)} –Ω–æ—Ä–º (—Ç–µ–∫—Å—Ç + —Ç–∞–±–ª–∏—Ü–∞). –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ {total} –±–∞—Ç—á–µ–π –ø–æ 8 –Ω–æ—Ä–º.\n")

    all_responses = []
    start_time = time.time()

    for i, batch in enumerate(batches, 1):
        print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i}/{total}... ", end="", flush=True)

        # –†–∞–∑–¥–µ–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∏ —Ç–∞–±–ª–∏—á–Ω—ã–µ –≤ –∫–∞–∂–¥–æ–º –±–∞—Ç—á–µ
        batch_text_norms = [t for t, _ in batch if t]
        batch_table_norms = [t for _, t in batch if t]

        prompt = generate_combined_prompt(batch_text_norms, batch_table_norms, fact_text)

        try:
            response = requests.post("http://localhost:11434/api/generate", json={
                "model": model_name,
                "prompt": prompt,
                "stream": False
            })
            response.raise_for_status()
            result = response.json().get("response", "‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏")
            result = clean_llm_output(result)
            all_responses.append(result.strip())
            
            print("‚úÖ –ì–æ—Ç–æ–≤–æ")
        except requests.exceptions.RequestException as e:
            all_responses.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏: {str(e)}")
            print("‚ùå –û—à–∏–±–∫–∞")

        if progress_bar and progress_label:
            percent = int((i / total) * 100)
            progress_bar.progress(i / total)
            progress_label.text(f"‚åõ –í—ã–ø–æ–ª–Ω–µ–Ω–æ: {percent}%")
    if progress_label:
        progress_label.text("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    elapsed = time.time() - start_time
    print(f"\nüèÅ –í—Å–µ –±–∞—Ç—á–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∑–∞ {round(elapsed, 2)} —Å–µ–∫.\n")
    print(f"–í–æ–ø—Ä–æ—Å –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞ {fact_text}")
    if all_responses:
        return clean_llm_output(summarize_llm_batches(all_responses, fact_text, model_name=model_name))
    else:
        return "‚ùó –ù–µ—Ç –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."


